{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "import webdataset as wds\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path pattern for your images (adjust as needed)\n",
    "IMAGE_PATTERN = \"path/to/images/**/*.jpg\"  \n",
    "# Path to your pseudo label list (one label per line)\n",
    "PSEUDO_LABELS_FILE = \"pseudo_labels.txt\"  \n",
    "# Output tar file for the WebDataset\n",
    "OUTPUT_TAR = \"pseudo_labeled_dataset.tar\"\n",
    "\n",
    "# --- Device selection ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load label strings ---\n",
    "with open(PSEUDO_LABELS_FILE, \"r\") as f:\n",
    "    labels = [line.strip() for line in f if line.strip()]\n",
    "num_labels = len(labels)\n",
    "print(f\"Loaded {num_labels} labels.\")\n",
    "\n",
    "# --- Load a pretrained CLIP model for pseudo labeling ---\n",
    "# (using OpenCLIPâ€™s \"ViT-B-32\" teacher model here)\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Precompute text embeddings for the 700 labels ---\n",
    "with torch.no_grad():\n",
    "    text_tokens = open_clip.tokenize(labels).to(device)\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features.cpu()\n",
    "\n",
    "# --- Prepare WebDataset writer ---\n",
    "writer = wds.TarWriter(OUTPUT_TAR)\n",
    "\n",
    "# --- Process each image ---\n",
    "image_paths = glob.glob(IMAGE_PATTERN, recursive=True)\n",
    "print(f\"Found {len(image_paths)} images.\")\n",
    "\n",
    "for image_path in image_paths:\n",
    "    try:\n",
    "        # Open and preprocess image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Compute image features\n",
    "        with torch.no_grad():\n",
    "            image_feature = model.encode_image(image_input)\n",
    "            image_feature /= image_feature.norm(dim=-1, keepdim=True)\n",
    "        image_feature = image_feature.cpu()\n",
    "\n",
    "        # Compute similarity scores against all label text embeddings\n",
    "        similarity = (image_feature @ text_features.t()).squeeze(0)  # (num_labels,)\n",
    "        best_idx = similarity.argmax().item()\n",
    "        best_label = labels[best_idx]\n",
    "        best_score = similarity[best_idx].item()\n",
    "\n",
    "        # Read raw image bytes (to save the original file)\n",
    "        with open(image_path, \"rb\") as img_f:\n",
    "            img_bytes = img_f.read()\n",
    "\n",
    "        # Create a record with a unique key\n",
    "        key = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        sample = {\n",
    "            \"__key__\": key,\n",
    "            \"jpg\": img_bytes,\n",
    "            \"cls\": best_label,  # you could also store the index if you prefer\n",
    "            \"json\": json.dumps({\"pseudo_label\": best_label, \"score\": best_score}),\n",
    "        }\n",
    "        writer.write(sample)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {image_path}: {e}\")\n",
    "\n",
    "writer.close()\n",
    "print(f\"Pseudo-labeled dataset saved to {OUTPUT_TAR}.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
